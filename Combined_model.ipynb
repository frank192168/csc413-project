{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XDfk1_P98n7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Model, GPT2Tokenizer, AdamW"
      ],
      "metadata": {
        "id": "AX3CBtv_I-F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/2023_residential_description.csv', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)\n",
        "    data_2023 = [row for row in reader]\n",
        "concat_col_2023 = np.array([f\"{row[0]} {row[1]} The sold price is \" for row in data_2023])"
      ],
      "metadata": {
        "id": "WC0kGZYdaBw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "class ResiDataset(Dataset):\n",
        "  def __init__(self, sentences, target, tokenizer):\n",
        "    self.sentences = sentences\n",
        "    self.target = target\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    encoding = self.tokenizer.encode_plus(self.sentences[idx], add_special_tokens=True, max_length=256, padding='max_length', pad_to_max_length=True,\n",
        "                        return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "    input = encoding['input_ids'].squeeze()\n",
        "    attention_mask = encoding['attention_mask'].squeeze()\n",
        "    target = torch.tensor(self.target[idx], dtype=torch.long)\n",
        "    return {'input_ids': input, 'attention_mask': attention_mask, 'target': target}"
      ],
      "metadata": {
        "id": "GgUYDx1aKzpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgc9GyBtIqGe"
      },
      "outputs": [],
      "source": [
        "gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
        "new_vocab_size = gpt2.config.vocab_size + 1 ## for <pad>\n",
        "new_embed = nn.Embedding(new_vocab_size, gpt2.config.hidden_size)\n",
        "new_embed.weight.data[:gpt2.config.vocab_size, :] = gpt2.wte.weight.data\n",
        "gpt2.set_input_embeddings(new_embed)\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, num_classes, gpt2):\n",
        "    super(GPTModel, self).__init__()\n",
        "    self.gpt_model = gpt2\n",
        "    self.fc = nn.Linear(gpt2.config.hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    gpt_out = self.gpt_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output_state = gpt_out.last_hidden_state.mean(dim=1)\n",
        "    return self.fc(pooled_output_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = GPTModel(801, gpt2)\n",
        "gpt_model.load_state_dict(torch.load('/content/drive/MyDrive/GPTModel.pth'))"
      ],
      "metadata": {
        "id": "AiKmpM8oJJ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in gpt_model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "dFy6jg-IYlz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt_feature(model, data):\n",
        "  encoding = tokenizer.encode_plus(data, add_special_tokens=True, max_length=256, padding='max_length', pad_to_max_length=True,\n",
        "                      return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "  input = encoding['input_ids'].squeeze()\n",
        "  attention_mask = encoding['attention_mask'].squeeze()\n",
        "  gpt_out = gpt_model.gpt_model(input_ids=input, attention_mask=attention_mask)\n",
        "  return gpt_out.last_hidden_state.mean(dim=1)"
      ],
      "metadata": {
        "id": "QuO4o2DFK6hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "wXRVry-y34yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "# for visualizing missing values\n",
        "import missingno as msno\n",
        "# from IPython.display import display\n",
        "# import dataframe_image as dfi\n",
        "\n",
        "# for display plot inline\n",
        "# %matplotlib inline\n",
        "# change the style\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import argparse\n",
        "import joblib"
      ],
      "metadata": {
        "id": "fkdt60F736Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(model_pred, target):\n",
        "\n",
        "    mae = mean_absolute_error(model_pred, target)\n",
        "    mse = mean_squared_error(model_pred, target, squared=True)\n",
        "\n",
        "    x = {\n",
        "        \"pred\": model_pred,\n",
        "        \"pred_y\": target\n",
        "    }\n",
        "    perc = np.abs(model_pred - target)/target\n",
        "    median = perc.median()\n",
        "    count_5 = perc[perc <= 0.05].count() / model_pred.size if model_pred.size else 0\n",
        "    count_10 = perc[perc <= 0.10].count() / model_pred.size if model_pred.size else 0\n",
        "    count_20 = perc[perc <= 0.20].count() / model_pred.size if model_pred.size else 0\n",
        "\n",
        "    # print(f\"{mae:.2f}\")\n",
        "    # # print(f\"{mse:.2f}\")\n",
        "    # print(f\"{np.sqrt(mse):.2f}\")\n",
        "    # print(\"median:\", median)\n",
        "    # print(\"<= 5%:\", count_5)\n",
        "    # print(\"<= 10%:\", count_10)\n",
        "    # print(\"<= 20%:\", count_20)\n",
        "\n",
        "    return {\n",
        "        \"mae\": mae,\n",
        "        \"mse\": mse,\n",
        "        \"rmse\": np.sqrt(mse),\n",
        "        \"median\": median,\n",
        "        \"count_5\": count_5,\n",
        "        \"count_10\": count_10,\n",
        "        \"count_20\": count_20\n",
        "    }\n",
        "\n",
        "def split_data(data, valid_perc=None, test_perc=None):\n",
        "    \"\"\"\n",
        "    Using np.split:\n",
        "    https://numpy.org/doc/stable/reference/generated/numpy.split.html\n",
        "    Assuming the dataset is sort by sold date desc. Using the latest for test, then for validation.\n",
        "    \"\"\"\n",
        "\n",
        "    if test_perc and valid_perc:\n",
        "        data_test, data_validate, data_train = np.split(data, [int(test_perc*len(data)), int((valid_perc+test_perc)*len(data))])\n",
        "    elif valid_perc:\n",
        "        data_validate, data_train = np.split(data, [int(valid_perc*len(data))])\n",
        "        data_test = None\n",
        "    else:\n",
        "        data_train = data\n",
        "        data_validate = None\n",
        "        data_test = None\n",
        "\n",
        "\n",
        "    data_train_x = data_train.drop(\"Sp_dol\", axis=1)\n",
        "    data_train_y = data_train[\"Sp_dol\"]\n",
        "\n",
        "    if valid_perc:\n",
        "        data_validate_x = data_validate.drop(\"Sp_dol\", axis=1)\n",
        "        data_validate_y = data_validate[\"Sp_dol\"]\n",
        "\n",
        "    if test_perc:\n",
        "        data_test_x = data_test.drop(\"Sp_dol\", axis=1)\n",
        "        data_test_y = data_test[\"Sp_dol\"]\n",
        "\n",
        "\n",
        "    print(\"     all data:\", data.shape)\n",
        "    print(   \"train data:\", data_train.shape)\n",
        "\n",
        "    if valid_perc:\n",
        "        print(\"validate data:\", data_validate.shape)\n",
        "\n",
        "    if test_perc:\n",
        "        print(\"    test data:\", data_test.shape)\n",
        "\n",
        "    result = {\n",
        "        # \"train\": data_train,\n",
        "        \"train_x\": data_train_x,\n",
        "        \"train_y\": data_train_y\n",
        "    }\n",
        "\n",
        "    if valid_perc:\n",
        "        result.update({\n",
        "            # \"validate\": data_validate,\n",
        "            \"validate_x\": data_validate_x,\n",
        "            \"validate_y\": data_validate_y,\n",
        "        })\n",
        "\n",
        "    if test_perc:\n",
        "        result.update({\n",
        "            # \"test\": data_test,\n",
        "            \"test_x\": data_test_x,\n",
        "            \"test_y\": data_test_y,\n",
        "        })\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def predict_result(data, target, predict, save_path=None):\n",
        "    \"\"\"\n",
        "    example usage:\n",
        "    result_res = predict_result(model_res, split_data_res[\"validate_x\"], split_data_res[\"validate_y\"], \"2023_res_validate_result.csv\")\n",
        "    \"\"\"\n",
        "\n",
        "    # pred = my_model.predict(d_x)\n",
        "    d_x = data\n",
        "    pred = predict\n",
        "    d_y = target\n",
        "\n",
        "    try:\n",
        "        d_x_index = d_x.index\n",
        "    except:\n",
        "        d_x_index = list(range(d_x.shape[0]))\n",
        "\n",
        "    output = pd.DataFrame({'Ml_num': d_x_index, 'predict': pred, \"actual\":d_y, \"diff\": pred - d_y, \"diff_perc\": round(np.abs(pred - d_y)/d_y, 4)})\n",
        "\n",
        "    if save_path:\n",
        "        output.to_csv(save_path, index=True)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def display_worst_prediction(data, target, predict, name=\"\", topk=10):\n",
        "\n",
        "    # pred = my_model.predict(d_x)\n",
        "    # pred_y = d_y\n",
        "    d_x = data\n",
        "    pred = predict\n",
        "    pred_y = target\n",
        "\n",
        "    try:\n",
        "        d_x_index = d_x.index\n",
        "    except:\n",
        "        d_x_index = list(range(d_x.shape[0]))\n",
        "\n",
        "    pred = pd.DataFrame({\"pred\":pred, \"Ml_num\":d_x_index, \"pred_y\":pred_y})\n",
        "    pred.set_index(\"Ml_num\", inplace=True)\n",
        "\n",
        "    pred[\"diff\"] = np.abs(pred[\"pred\"]-pred[\"pred_y\"]) / pred[\"pred_y\"]\n",
        "\n",
        "    res = pred[[\"pred\", \"pred_y\", \"diff\"]].sort_values(by=[\"diff\"],ascending=False) \\\n",
        "            .head(topk).rename(columns={\"pred\": \"Prediction\", \"pred_y\": \"Sale Price\", \"diff\":\"Different Percentage\"}) \\\n",
        "            .style.format({\"Prediction\":\"{:,.0f}\", \"Sale Price\":\"{:,.0f}\", \"Different Percentage\":\"{:,.2%}\"}) \\\n",
        "            .set_table_styles([{\n",
        "                 'selector': 'caption',\n",
        "                 'props': 'font-weight:bold;font-size:1.25em;'\n",
        "             }], overwrite=False) \\\n",
        "            .set_caption(f\"Top {topk} Worst Predict Result of Listings\" + (\"\" if not name else f\"<br>{name}\"))\n",
        "            # .set_caption(f\"Worst Predict Result of Listings<br>({data_name} set)\"))\n",
        "\n",
        "\n",
        "    # print(\"Total predicted:\", pred.shape)\n",
        "    # print(\"Total predicted with difference > 0.5:\", pred[pred[\"diff\"]>0.5].shape)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def display_predict_result(data, target, predict, name=\"\", group_by=\"Area\", sort_by=\"Homes\", ascending=False):\n",
        "\n",
        "\n",
        "    # pred = my_model.predict(d_x)\n",
        "    # pred_y = d_y\n",
        "    d_x = data\n",
        "    pred = predict\n",
        "    pred_y = target\n",
        "\n",
        "    # group_by = \"Area\" #\"Area\" # \"Municipality_district\" # \"S_r\"\n",
        "    # sort_by = \"Homes\" # \"Municipality_district\" # \"Homes\"\n",
        "    # ascending = False\n",
        "\n",
        "    try:\n",
        "        d_x_index = d_x.index\n",
        "    except:\n",
        "        d_x_index = list(range(d_x.shape[0]))\n",
        "\n",
        "    pred = pd.DataFrame({\"pred\":pred, \"Ml_num\":d_x_index, \"pred_y\":pred_y})\n",
        "    pred = pd.concat([pred, d_x[[group_by]]], axis=1, join='inner')\n",
        "    pred.set_index(\"Ml_num\", inplace=True)\n",
        "\n",
        "\n",
        "    def calculation(x):\n",
        "        perc = np.abs(x[\"pred\"] - x[\"pred_y\"])/x[\"pred_y\"]\n",
        "        median = perc.median()\n",
        "        count_5 = perc[perc <= 0.05].count() / x[\"pred\"].size if x[\"pred\"].size else 0\n",
        "        count_10 = perc[perc <= 0.10].count() / x[\"pred\"].size if x[\"pred\"].size else 0\n",
        "        count_20 = perc[perc <= 0.20].count() / x[\"pred\"].size if x[\"pred\"].size else 0\n",
        "\n",
        "        # print(x.shape) <= (15, 3)\n",
        "        # print(x.size) <= 45\n",
        "        # print(x.size.astype(int)) <= 45\n",
        "\n",
        "        res = {'Median Error': median, 'Within 5% of Sales Price': count_5, 'Within 10% of Sales Price': count_10, 'Within 20% of Sales Price': count_20, \"Homes\":x[\"pred\"].size}\n",
        "        return pd.Series(res, index=res.keys())\n",
        "\n",
        "\n",
        "    # TODO: why groupby contains empty dataframe, a workaround right now is prevent zero division in calculation()\n",
        "    result = pred.groupby([group_by]).apply(calculation)\n",
        "    result.loc[\"All Areas\"] = calculation(pred)\n",
        "\n",
        "    result = result.sort_values(by=sort_by, ascending=ascending)\n",
        "    result_style = result.style.format({'Median Error': \"{:.2%}\",'Within 5% of Sales Price': \"{:.2%}\",'Within 10% of Sales Price': \"{:.2%}\",'Within 20% of Sales Price': \"{:.2%}\",'Homes': \"{:,.0f}\"}) \\\n",
        "                    .set_table_styles([{\n",
        "                                 'selector': 'caption',\n",
        "                                 'props': 'font-weight:bold;font-size:1.25em;'\n",
        "                             }], overwrite=False) \\\n",
        "                    .set_caption(name)\n",
        "    # display(result_style)\n",
        "    # result[['Median Error','Within 5% of Sales Price','Within 10% of Sales Price','Within 20% of Sales Price']] = result[['Median Error','Within 5% of Sales Price','Within 10% of Sales Price','Within 20% of Sales Price']].applymap('{:.2%}'.format)\n",
        "    # result['Homes'] = result['Homes'].apply('{:,.0f}'.format)\n",
        "    # result.dfi.export(f\"2023_res_validate_result.png\")\n",
        "\n",
        "    return result_style"
      ],
      "metadata": {
        "id": "vZnncrxv4dha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_lot_size(x):\n",
        "\n",
        "    if x[\"Front_ft\"] == 0:\n",
        "        val = x[\"Depth\"]\n",
        "    elif x[\"Depth\"] == 0:\n",
        "        val = x[\"Front_ft\"]\n",
        "    else:\n",
        "        val = x[\"Front_ft\"] * x[\"Depth\"]\n",
        "\n",
        "    if not val:\n",
        "        return val\n",
        "\n",
        "    # convert all units to Feet\n",
        "    if x[\"Lotsz_code\"] == \"Feet\":\n",
        "        return val\n",
        "    elif x[\"Lotsz_code\"] == \"Metres\":\n",
        "        return val * 3.28084\n",
        "    elif x[\"Lotsz_code\"] == \"Acres\":\n",
        "        return val * 43560\n",
        "    else:\n",
        "        return val\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    data = data.dropna(subset=['Sp_dol'])\n",
        "\n",
        "    # convert fields to boolean\n",
        "    #   \"Den_fr\": \"Family Room - not NA: 47287/50026 - 94.52% - ['N', 'Y']\",\n",
        "    data['custom_den_fr'] = data['Den_fr'].apply(lambda x: True if x=='Y' else False)\n",
        "    data['Taxes'] = data['Taxes'].apply(lambda x: None if x==0 else x)\n",
        "    data['Tv'] = data['Tv'].apply(lambda x: None if x==0 else x)\n",
        "    data['custom_tour_url'] = data['Tour_url'].apply(lambda x: True if type(x) == str and x.strip() else False)\n",
        "    data['custom_fpl_num'] = data['Fpl_num'].apply(lambda x: True if x=='Y' else False)\n",
        "\n",
        "    data['Lat'] = data['Lat'].apply(lambda x: None if x==0 else x)\n",
        "    data['Lng'] = data['Lng'].apply(lambda x: None if x==0 else x)\n",
        "\n",
        "    # new fields for special handle fields\n",
        "    data[\"custom_lot_size\"] = data.apply(lambda x:handle_lot_size(x), axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def convert_datatype(data):\n",
        "    \"\"\"\n",
        "    This supposes split all features into each datatype, and select by user later. But we only pick\n",
        "    those we may care about.\n",
        "    \"\"\"\n",
        "\n",
        "    numerics_int_res = [\n",
        "        \"Photo_count\",\n",
        "        \"Bath_tot\", \"Br\", \"Br_plus\", \"Rms\", \"Rooms_plus\", \"Kit_plus\", \"Num_kit\",\n",
        "        \"Gar_spaces\", \"Park_spcs\",\n",
        "        # \"Lp_dol\",\n",
        "        \"Sp_dol\", # target\n",
        "    ]\n",
        "\n",
        "    numerics_float_res = [\n",
        "        \"Lat\", \"Lng\",\n",
        "        # \"Taxes\",\n",
        "        \"custom_lot_size\",\n",
        "    ]\n",
        "\n",
        "    dates_res = [\"Input_date\"] # \"Input_date\" makes it worse, should we shuffle the data? Right now it's sorted by Cd (sold date)\n",
        "\n",
        "    bools_res = [\"custom_den_fr\", \"custom_tour_url\", \"custom_fpl_num\"]\n",
        "\n",
        "    categories_res = [\n",
        "        \"Comp_pts\", # unique: 4\n",
        "        \"Constr1_out\", \"Constr2_out\", # todo: they represent the same thing, (e.g. A,B = B,A) # unique: 14\n",
        "        \"Bsmt1_out\", \"Bsmt2_out\", # todo: they represent the same thing, (e.g. A,B = B,A) # unique: 14\n",
        "        \"Yr_built\", # ['0-5', '100+', '16-30', '31-50', '51-99', ...] unique: 7\n",
        "        \"Acres\", # unique: 9\n",
        "        \"Sqft\", # unique: 9\n",
        "        \"Style\", # unique: 17\n",
        "        \"Type_own1_out\", # unique: 17\n",
        "        # \"Spec_des1_out\", # unique 6, though almost all are Unknown (46070 over 50026)\n",
        "        \"Area\", # unique 7 (we restricted the records to 7 areas)\n",
        "        \"Municipality_district\", # unique 86 (within these 7 areas)\n",
        "        \"Community\", # unique 579 (within these 7 areas)\n",
        "    ] # worse: \"Sewer\", \"Heating\",\n",
        "\n",
        "    # features_res = numerics_float_res + numerics_int_res + bools_res + categories_res + dates_res\n",
        "    # print(\"features:\", len(features_res))\n",
        "\n",
        "    # TODO: handle: Zip?, Input_date?\n",
        "\n",
        "    for num in numerics_float_res:\n",
        "        data[num] = data[num].fillna(0).astype(float)\n",
        "\n",
        "    for num in numerics_int_res:\n",
        "        data[num] = data[num].fillna(0).round().astype('int64')\n",
        "\n",
        "\n",
        "    for category in categories_res:\n",
        "        data[category] = data[category].astype(\"category\")\n",
        "\n",
        "    for d in dates_res:\n",
        "        # data[d] = data[d].apply(lambda x: x.Timestamp.value)\n",
        "        # data[d] = data[d].dt.strftime(\"%Y%m%d\").astype(int)\n",
        "        data[d] = data[d].str.replace(\"-\",\"\").fillna(0).astype(int)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def export_data(data, config):\n",
        "\n",
        "    feature_lis = []\n",
        "    for k,v in config[\"features\"].items():\n",
        "        feature_lis.extend(v)\n",
        "\n",
        "    data = data[feature_lis]\n",
        "\n",
        "    if \"testset_percentage\" in config and config[\"testset_percentage\"]:\n",
        "        data_test, data_train = np.split(data, [int(config[\"testset_percentage\"]*len(data))])\n",
        "        data_train.to_csv(config[\"save_path\"])\n",
        "        data_test.to_csv(config[\"save_path\"].replace(\".csv\", \"_test.csv\"))\n",
        "        print(f\"Training size (without index): {data_train.shape}\")\n",
        "        print(f\"Test size (without index): {data_test.shape}\")\n",
        "    else:\n",
        "        data.to_csv(config[\"save_path\"])\n",
        "        print(f\"Training size (without index): {data.shape}\")\n",
        "\n",
        "    # save metadata\n",
        "    with open(config[\"save_path\"].replace(\".csv\", \".json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(config, indent=2, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def select_features(dataset, metadata):\n",
        "    numerics_int = metadata[\"features\"][\"integer\"]\n",
        "    numerics_float = metadata[\"features\"][\"float\"]\n",
        "    numerics_bool = metadata[\"features\"][\"boolean\"]\n",
        "    categories = metadata[\"features\"][\"category\"]\n",
        "\n",
        "    for num in numerics_float:\n",
        "        dataset[num] = dataset[num].fillna(0).astype(float)\n",
        "\n",
        "    for num in numerics_int:\n",
        "        dataset[num] = dataset[num].fillna(0).round().astype('int64')\n",
        "\n",
        "    for num in numerics_bool:\n",
        "        dataset[num] = dataset[num].astype('bool')\n",
        "\n",
        "    for category in categories:\n",
        "        dataset[category] = dataset[category].astype(\"category\")\n",
        "\n",
        "    features = []\n",
        "    for k,v in metadata[\"features\"].items():\n",
        "        features.extend(v)\n",
        "\n",
        "\n",
        "    return dataset[features]"
      ],
      "metadata": {
        "id": "Z0DD0cnd5B6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/features.json', 'r') as f:\n",
        "  features = json.load(f)\n",
        "mlp_model = torch.jit.load(\"/content/drive/MyDrive/mlp_model.pt\")\n",
        "mlp_pipeline = joblib.load(\"/content/drive/MyDrive/pipeline.pkl\")\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2023_Residential_extra.csv\")\n",
        "df.set_index('Ml_num', inplace = True)\n",
        "df = df.sort_values(by='Cd',ascending=False)\n",
        "\n",
        "df = preprocessing(df)\n",
        "df = convert_datatype(df)\n",
        "df = select_features(df, features)\n",
        "\n",
        "train_x = df.drop(\"Sp_dol\", axis=1)\n",
        "train_y = df[\"Sp_dol\"]\n"
      ],
      "metadata": {
        "id": "Z2tS9Xxd-dau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mlp_feature(model, train_x, pipeline):\n",
        "  a = pipeline.transform(train_x) # train_x here is one data row\n",
        "  a = torch.tensor(a.toarray(), dtype=torch.float32)\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  return model.features(a.to(device))"
      ],
      "metadata": {
        "id": "I1l57JYqcJEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinalMLP(nn.module):\n",
        "  def __init__(self, concat_feature_len, num_hidden):\n",
        "    self.fc1 = nn.Linear(concat_feature_len, 256)\n",
        "    self.fc2 = nn.Linear(256, 64)\n",
        "\n",
        "  def forward(self, gpt_feature, mlp_feature, cnn_feature):\n",
        "    concat_feature = np.concatenate((gpt_feature, mlp_feature, cnn_feature))\n",
        "    h1 = self.fc1(concat_feature)\n",
        "    z1 = torch.relu(h1)\n",
        "    return self.fc2(z1)"
      ],
      "metadata": {
        "id": "BfPBHPWWReuJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}