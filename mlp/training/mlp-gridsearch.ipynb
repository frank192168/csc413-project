{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cda4c8c-2a22-4d87-a33f-5128eb88f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from helper_functions import utils\n",
    "from IPython.display import display\n",
    "import dataframe_image as dfi\n",
    "import joblib # for save pipeline\n",
    "\n",
    "# for display plot inline\n",
    "%matplotlib inline\n",
    "# change the style\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9a924-1588-42d9-95b6-a833d425e812",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80b2dd0-39e7-453a-b60f-80b92dc144dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "\n",
    "dataset_f = '../data/dataset/2023_Residential_extra_preprocessed.csv'\n",
    "metadata_f = '../data/dataset/2023_Residential_extra_preprocessed.json'\n",
    "\n",
    "# This is for saving the prediciton results\n",
    "output_dir = Path(f'./results/mlp/2023_Residential_extra_preprocessed_gridsearch/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "figure_name = \"MLP\"\n",
    "# figure_name = \"XGBoost<br>(no extra)\"\n",
    "\n",
    "remove_features = [\n",
    "    # estimation\n",
    "    # \"Lp_dol\", \"Taxes\",\n",
    "    # school\n",
    "    # \"s_2km_total\", \"s_2km_public\", \"s_2km_private\", \"s_2km_catholic\", \"s_2km_ele\", \"s_2km_sec\", \"s_2km_hi_ele_score\", \"s_2km_hi_sec_score\", \"s_2km_avg_ele_score\", \"s_2km_avg_sec_score\",\n",
    "    # # census\n",
    "    # \"labour_force_perc\", \"avg_household_income\", \"owned_perc\", \"with_children_perc\", \"households\", \"married__perc\", \"single__perc\", \"university_degree_perc\", \"median_age\", \"population\", \"age_0_9_perc\", \"age_10_19_perc\", \"age_20_34_perc\", \"age_35_64_perc\",\n",
    "    # # market\n",
    "    # \"average_sold_price\", \"average_listing_price\", \"median_sold_price\", \"median_listing_price\", \"average_days_on_market\", \"median_days_on_market\", \"dollar_volume\", \"new_listing_count\", \"sold_listing_count\", \"sold_overasking_count\", \"sold_underasking_count\", \"active_listing_count\", \"sales_to_new_listing_ratio\", \"months_of_inventory\",\n",
    "    # # transit\n",
    "    # \"ttc_subway_1km\", \"go_train_1km\", \"trainsit_1km\",\n",
    "]\n",
    "\n",
    "dataset = pd.read_csv(dataset_f)\n",
    "dataset.set_index('Ml_num', inplace = True)\n",
    "\n",
    "with open(metadata_f, \"r\") as f:\n",
    "    metadata = json.loads(f.read())\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, \"remove_features.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(remove_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758d2d2-09ff-461e-8a71-d034596f4939",
   "metadata": {},
   "source": [
    "# Handle datatype res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ee7009-85e9-4255-afc5-379c24d8d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics_int = metadata[\"features\"][\"integer\"]\n",
    "numerics_float = metadata[\"features\"][\"float\"]\n",
    "numerics_bool = metadata[\"features\"][\"boolean\"]\n",
    "categories = metadata[\"features\"][\"category\"]\n",
    "    \n",
    "\n",
    "for num in numerics_float:\n",
    "    dataset[num] = dataset[num].fillna(0).astype(float)\n",
    "    \n",
    "for num in numerics_int:\n",
    "    dataset[num] = dataset[num].fillna(0).round().astype('int64')\n",
    "    \n",
    "for num in numerics_bool:\n",
    "    dataset[num] = dataset[num].astype('long')\n",
    "\n",
    "for category in categories:\n",
    "    dataset[category] = dataset[category].astype(\"category\")\n",
    "\n",
    "\n",
    "features = []\n",
    "for k,v in metadata[\"features\"].items():\n",
    "    features.extend(v)\n",
    "\n",
    "\n",
    "features_set = set(features)\n",
    "for feat in remove_features:\n",
    "    if feat in features_set:\n",
    "        features.remove(feat)\n",
    "\n",
    "    \n",
    "dataset = dataset[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26923f5-e702-40ca-b812-71cd60e792f9",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b87911e-6251-4149-8c95-353f357cfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerics_int = metadata[\"features\"][\"integer\"]\n",
    "numerics_float = metadata[\"features\"][\"float\"]\n",
    "numerics_bool = metadata[\"features\"][\"boolean\"]\n",
    "categories = metadata[\"features\"][\"category\"]\n",
    "\n",
    "target = 'Sp_dol'\n",
    "\n",
    "numerical_features = numerics_int + numerics_float + numerics_bool\n",
    "categorical_features = categories\n",
    "\n",
    "# Just in case the target value in features\n",
    "numerical_features.remove(target) if target in numerical_features else None\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "X_full = dataset[numerical_features + categorical_features]\n",
    "y_full = dataset[target]\n",
    "\n",
    "# pipeline should only apply on training set, not on validation set\n",
    "# X = pipeline.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895ba8dc-d115-4ecc-a3e4-156d883ca872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, X_, X, y, save_dir):\n",
    "\n",
    "    # X.set_index('Ml_num', inplace = True)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # X_tensor = torch.tensor(X_val.toarray()).float().to(device)\n",
    "        # y_tensor = torch.tensor(y_val).float().unsqueeze(1).to(device)\n",
    "        \n",
    "        model_pred = model(torch.tensor(X.toarray()).float().to(device))\n",
    "\n",
    "    model_pred = model_pred.view(-1).cpu().detach().numpy()\n",
    "    # X_, y_ = X.cpu().detach().numpy(), y.view(-1).cpu().detach().numpy()\n",
    "    # X_.set_index('Ml_num', inplace = True)\n",
    "    \n",
    "    \n",
    "    utils.predict_result(X_, y, model_pred, os.path.join(save_dir, \"validate_result.csv\"))\n",
    "    \n",
    "    style_worst = utils.display_worst_prediction(X_, y, model_pred, figure_name)\n",
    "    dfi.export(style_worst, os.path.join(save_dir, 'top_worst_predictions.png'))\n",
    "\n",
    "    style_pred_area = utils.display_predict_result(X_, y, model_pred, figure_name, group_by=\"Area\", sort_by=\"Homes\", ascending=False)\n",
    "    dfi.export(style_pred_area, os.path.join(save_dir, 'predictions_by_area.png'))\n",
    "\n",
    "    style_pred_muni = utils.display_predict_result(X_, y, model_pred, figure_name, group_by=\"Municipality_district\", sort_by=\"Homes\", ascending=False)\n",
    "    dfi.export(style_pred_muni, os.path.join(save_dir, 'predictions_by_municipality.png'))\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_sizes, dropout_rates):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, layer_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rates[0]),\n",
    "            nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rates[1]),\n",
    "            nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_sizes[2], 1)\n",
    "            # LeakyReLU\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=5, plot_every=100, plot=True, save_path=\"\"):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_mae = []\n",
    "    val_mae = []\n",
    "    train_mse = []\n",
    "    val_mse = []\n",
    "    train_rmse = []\n",
    "    val_rmse = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            train_loss.append(float(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # train_mae.append(evaluate_model(model, train_loader)[0])\n",
    "        # val_mae.append(evaluate_model(model, val_loader)[0])\n",
    "        mae, mse, rmse = evaluate_model(model, train_loader)\n",
    "        train_mae.append(mae)\n",
    "        train_mse.append(mse)\n",
    "        train_rmse.append(rmse)\n",
    "        mae, mse, rmse = evaluate_model(model, val_loader)\n",
    "        val_mae.append(mae)\n",
    "        val_mse.append(mse)\n",
    "        val_rmse.append(rmse)\n",
    "\n",
    "    if plot:\n",
    "        every_n_train_loss = train_loss[0::plot_every]\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(list(map(lambda x:x*plot_every, range(1,len(every_n_train_loss)+1))), every_n_train_loss, label='Loss')\n",
    "        plt.title(f\"Loss over Every {plot_every} Iterations\")\n",
    "        plt.xlabel(f\"Every {plot_every} Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.savefig(os.path.join(save_path, 'loss.png'))\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # # Plotting the MAE\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_mae)+1), train_mae, label='Train MAE')\n",
    "        plt.plot(range(1, len(val_mae)+1), val_mae, label='Val MAE')\n",
    "        plt.xlabel(f'{epochs} Epochs')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Mean Absolute Error Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_path, 'mae.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # # Plotting the MSE\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_mse)+1), train_mse, label='Train MSE')\n",
    "        plt.plot(range(1, len(val_mse)+1), val_mse, label='Val MSE')\n",
    "        plt.xlabel(f'{epochs} Epochs')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.title('Mean Squared Error Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_path, 'mse.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        # # Plotting the RMSE\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_rmse)+1), train_rmse, label='Train RMSE')\n",
    "        plt.plot(range(1, len(val_rmse)+1), val_rmse, label='Val RMSE')\n",
    "        plt.xlabel(f'{epochs} Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('Root Mean Squared Error Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_path, 'rmse.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# Evaluation function to calculate MAE, MSE, and RMSE\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.view(-1).tolist())\n",
    "            actuals.extend(targets.view(-1).tolist())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return mae, mse, rmse\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation over the hyperparameters\n",
    "def grid_search_cv(hyperparams, X, y, n_splits=5, epochs=10):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    print_results = []\n",
    "\n",
    "    for num_neurons, lr, batch_size, dropout_rates in product(hyperparams['num_neurons'], hyperparams['learning_rate'], hyperparams['batch_size'], hyperparams['dropout_rates']):\n",
    "        fold_results = []\n",
    "\n",
    "        save_path = Path(os.path.join(output_dir, f\"Neurons {num_neurons}, LR {lr}, Batch {batch_size}, Dropout {dropout_rates} Epochs {epochs}/\"))\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            X_train_, X_val_ = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # apply transform pipline\n",
    "            X_train, X_val = pipeline.fit_transform(X_train_), pipeline.transform(X_val_)\n",
    "\n",
    "            train_dataset = TensorDataset(torch.tensor(X_train.toarray()).float(), torch.tensor(y_train).float().unsqueeze(1))\n",
    "            val_dataset = TensorDataset(torch.tensor(X_val.toarray()).float(), torch.tensor(y_val).float().unsqueeze(1))\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = MLPModel(input_dim=X_train.shape[1], layer_sizes=num_neurons, dropout_rates=dropout_rates).to(device)\n",
    "            loss_function = nn.MSELoss() #nn.L1Loss() # nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            save_path_fold = Path(os.path.join(save_path, f\"Fold {fold}/\"))\n",
    "            save_path_fold.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=epochs, save_path=save_path_fold)\n",
    "\n",
    "            model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "            model_scripted.save(os.path.join(save_path_fold, \"mlp_model.pt\")) # Save\n",
    "            joblib.dump(pipeline, os.path.join(save_path_fold, 'pipeline.pkl'))\n",
    "            \n",
    "            mae, mse, rmse = evaluate_model(model, val_loader)\n",
    "\n",
    "            get_results(model, dataset.iloc[val_idx], X_val, y_val, save_path_fold)\n",
    "            \n",
    "            fold_results.append((mae, mse, rmse))\n",
    "        \n",
    "        avg_results = np.mean(fold_results, axis=0)\n",
    "        results.append((num_neurons, lr, batch_size, dropout_rates, avg_results.tolist()))\n",
    "\n",
    "        print_result = f\"Neurons: {num_neurons}, LR: {lr}, Batch: {batch_size}, Dropout: {dropout_rates} Loss: MSELoss, Optimizer: Adam, Avg MAE: {avg_results[0]:.4f}, Avg MSE: {avg_results[1]:.4f}, Avg RMSE: {avg_results[2]:.4f} Time: {time.time()-start_time:.2f} seconds\"\n",
    "        print_results.append(print_result)\n",
    "        print(print_result)\n",
    "\n",
    "    with open(os.path.join(output_dir, \"result.json\"), \"w\") as f:\n",
    "        f.write(json.dumps(results, ensure_ascii=False))\n",
    "\n",
    "    with open(os.path.join(output_dir, \"result.txt\"), \"w\") as f:\n",
    "        for txt in print_results:\n",
    "            f.write(txt + \"\\n\")\n",
    "\n",
    "    # Find the best parameters based on the average RMSE\n",
    "    best_params = min(results, key=lambda x: x[4][2])[:4]  # Get the params part of the tuple based on RMSE\n",
    "    return best_params, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4856f85-8630-4031-a241-f60db63a3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: (128, 64, 32), LR: 0.01, Batch: 64, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 74965.7690, Avg MSE: 13704241571.2410, Avg RMSE: 116989.4553 Time: 186.75 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.01, Batch: 64, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 76474.9845, Avg MSE: 14502103435.0723, Avg RMSE: 120241.6843 Time: 371.95 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.01, Batch: 128, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 74401.4551, Avg MSE: 13848495198.7452, Avg RMSE: 117605.5463 Time: 500.97 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.01, Batch: 128, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 74162.4871, Avg MSE: 14049339214.9273, Avg RMSE: 118457.7167 Time: 639.00 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.001, Batch: 64, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 98520.3160, Avg MSE: 33052798933.5439, Avg RMSE: 180570.0567 Time: 826.96 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.001, Batch: 64, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 93542.6071, Avg MSE: 28892166752.5861, Avg RMSE: 169077.8751 Time: 1011.74 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.001, Batch: 128, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 143267.8566, Avg MSE: 76391314995.8011, Avg RMSE: 275995.3146 Time: 1140.03 seconds\n",
      "Neurons: (128, 64, 32), LR: 0.001, Batch: 128, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 139722.3814, Avg MSE: 71572503499.3049, Avg RMSE: 266861.0713 Time: 1264.64 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.01, Batch: 64, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 79496.9881, Avg MSE: 14901458101.6839, Avg RMSE: 121969.2644 Time: 1440.55 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.01, Batch: 64, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 77298.9103, Avg MSE: 14513675361.7677, Avg RMSE: 120465.6625 Time: 1617.27 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.01, Batch: 128, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 85418.2869, Avg MSE: 17878551934.6178, Avg RMSE: 132674.6012 Time: 1741.21 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.01, Batch: 128, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 75809.3176, Avg MSE: 14152157091.4586, Avg RMSE: 118813.0933 Time: 1866.18 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.001, Batch: 64, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 76692.0632, Avg MSE: 15343126337.3746, Avg RMSE: 123794.7827 Time: 2036.37 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.001, Batch: 64, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 76041.6137, Avg MSE: 15334790331.9297, Avg RMSE: 123704.3872 Time: 2206.20 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.001, Batch: 128, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 96213.0661, Avg MSE: 29615244009.9042, Avg RMSE: 171445.0276 Time: 2328.42 seconds\n",
      "Neurons: (256, 128, 64), LR: 0.001, Batch: 128, Dropout: (0.5, 0.5) Loss: MSELoss, Optimizer: Adam, Avg MAE: 92066.1629, Avg MSE: 26623739291.3297, Avg RMSE: 162305.0602 Time: 2453.52 seconds\n",
      "Best Params based on RMSE: ((128, 64, 32), 0.01, 64, (0.2, 0.2))\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'num_neurons': [(128, 64, 32), (256, 128, 64)],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'batch_size': [64, 128],\n",
    "    'dropout_rates': [(0.2, 0.2), (0.5, 0.5)]\n",
    "} # 20 mins: 2x2x2x2\n",
    "\n",
    "# hyperparameters = {\n",
    "#     'num_neurons': [(256, 128, 64)],\n",
    "#     'learning_rate': [0.01],\n",
    "#     'batch_size': [128],\n",
    "#     'dropout_rates': [(0.2, 0.2)]\n",
    "# }\n",
    "epochs = 10\n",
    "best_params, results = grid_search_cv(hyperparameters, X_full, y_full, epochs=epochs)\n",
    "print(\"Best Params based on RMSE:\", best_params)\n",
    "\n",
    "with open(os.path.join(output_dir, \"best_result.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(best_params, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e3a89a-9e0c-4082-b551-f54116f33648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: (128, 64, 32), LR: 0.01, Batch: 64, Dropout: (0.2, 0.2) Loss: MSELoss, Optimizer: Adam, Avg MAE: 77549.6466, Avg MSE: 15721602378.5215, Avg RMSE: 125385.8141 Time: 36.00 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val_, y_train, y_val = train_test_split(X_full, y_full, test_size=0.2, shuffle=False)\n",
    "\n",
    "num_neurons, lr, batch_size, dropout_rates = best_params\n",
    "\n",
    "# apply transform pipline\n",
    "X_train, X_val = pipeline.fit_transform(X_train), pipeline.transform(X_val_)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train.toarray()).float(), torch.tensor(y_train).float().unsqueeze(1))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val.toarray()).float(), torch.tensor(y_val).float().unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = MLPModel(input_dim=X_train.shape[1], layer_sizes=num_neurons, dropout_rates=dropout_rates).to(device)\n",
    "loss_function = nn.MSELoss() #nn.L1Loss() # nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "save_path = Path(os.path.join(output_dir, f\"best_model/\"))\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=epochs, save_path=save_path)\n",
    "\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save(os.path.join(save_path, \"mlp_model.pt\")) # Save\n",
    "joblib.dump(pipeline, os.path.join(save_path, 'pipeline.pkl'))\n",
    "\n",
    "mae, mse, rmse = evaluate_model(model, val_loader)\n",
    "\n",
    "get_results(model, X_val_, X_val, y_val, save_path)\n",
    "\n",
    "print_result = f\"Neurons: {num_neurons}, LR: {lr}, Batch: {batch_size}, Dropout: {dropout_rates} Loss: MSELoss, Optimizer: Adam, Avg MAE: {mae:.4f}, Avg MSE: {mse:.4f}, Avg RMSE: {rmse:.4f} Time: {time.time()-start_time:.2f} seconds\"\n",
    "print(print_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5553b6-503d-4f39-9fb5-eef5d55a879e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
